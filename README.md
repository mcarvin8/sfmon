# Salesforce Monitoring (SFMon)

SFMon is an AWS-hosted service which runs a Docker container that scrapes Salesforce org metrics and exposes them to Prometheus for observability and alerting.

This project provides the necessary resources to deploy a Salesforce Monitoring (SFMon) service on AWS ECS.

A Grafana dashboard can be used to visualize the Prometheus targets created by SFMon.

While this project provides the files to run on AWS ECS, you could run the custom Docker container on other cloud platforms.
---

## üöÄ Quick Start

1. **Provision AWS Infrastructure** ‚Äì ECS Cluster, ECR, Prometheus service, IAM roles, security groups.
2. **Build & Push Docker Image** ‚Äì Include your Salesforce auth URLs as build arguments.
3. **Deploy to ECS** ‚Äì Use Terraform to launch the service into your cluster.
4. **Configure Grafana** ‚Äì Import the sample dashboard and start visualizing metrics.

---

## Overview

SFMon runs as an ECS service using a Docker image that contains the required Python environment and Salesforce dependencies (`Salesforce CLI`). It periodically authenticates with your Salesforce orgs and collects monitoring data.

---

## Authors

Originally developed by Deep Suthar and Matt Carvin.

---

## Prerequisites

Before you can deploy SFMon ECS, you must provision the following AWS infrastructure:

- An ECS Cluster to host the SFMon service  
  _(you can deploy one using `sre/deployments/terraform/ecs-cluster`)_
- A Prometheus ECS Service to scrape and store metrics  
  _(deploy using `sre/deployments/terraform/prometheus-service`)_
- The ECS Exec Command IAM policy to allow container access  
  _(deploy using `sre/deployments/terraform/ecs-exec-command-policy`)_
- `sre/deployments/terraform/sfmon-service/prereqs` includes:
  - ECR for SFMon images
  - IAM role for SFMon
  - Security Groups for SFMon
  - CloudWatch log group for SFMon

---

## Modifying Scripts

The Python scripts that run the monitoring service are located in `sre/deployments/scripts/sfmon-service`.

The scripts are separated into 2 folders for 2 ECS services. `prd` for production monitoring and `sbx` for sandbox monitoring. There's overlap in some monitoring between production and sandboxes, which you can obviously tailor for your specific orgs.

The primary scheduler script is `salesforce_monitoring.py`, which the Docker image runs at launch. This script:

- Imports and schedules monitoring functions
- Authenticates to each Salesforce org via Salesforce CLI
- Runs each monitoring function on a defined schedule

To customize for your orgs:

- Update the SFDX authorization URLs (see Notes below)
- Remove monitoring functions you don't need
- Add or customize monitoring functions for your use case
- Adjust execution intervals as needed

üìå **Examples of current monitoring functions:**

- Incidents (all orgs)
- API limits (production)
- License counts/limits (production)
- Bulk API analysis details (production)
- Deployment and validation tracking (production)
- Performance metrics (production)
- Errors (production)
- User login metrics (production)
- Compliance panels (production)
- Email deliverability settings (sandboxes)
- Payment gateway and method health checks (full-copy sandboxes)

Currently, the production org is the primary monitoring target, with 3 sandboxes monitored for specific configuration or incident-related checks.

---

## Building and Publishing the Docker Image

SFMon depends on a custom Docker image that must be built and pushed to your ECR.  
**The image exposes Port 9001 for Prometheus metrics.**

When building the image, you must provide the SFDX authorization URLs for each Salesforce org you intend to monitor. These are the `sfdx auth:sfdxurl:store` or web login URLs generated by the Salesforce CLI.

Example Docker build and push commands:

```
# Set ECR repo URL and AWS region variables
TF_VAR_ecr_repo="${ECR_REPO}"
TF_VAR_aws_region="${AWS_REGION}"

# Login to ECR using Terraform
cd sre/deployments/terraform/docker-login
terraform init -input=false
terraform apply -input=false -auto-approve

# Build Docker image with auth URLs as build arguments
docker build \
  --file "./sre/deployments/docker/sfmon-service/Dockerfile" \
  --build-arg PRODUCTION_AUTH_URL=$PRODUCTION_AUTH_URL \
  --build-arg FULLQA_AUTH_URL=$FULLQA_AUTH_URL \
  --build-arg FULLQAB_AUTH_URL=$FULLQAB_AUTH_URL \
  --build-arg DEV_AUTH_URL=$DEV_AUTH_URL \
  --tag $ECR_REPO:$CI_COMMIT_SHORT_SHA .

# Push image to ECR
docker push $ECR_REPO:$CI_COMMIT_SHORT_SHA
```

## Deploying SFMon to ECS

Once your image is published and infrastructure is ready:

1. Create or update your ECS Task Definition to use the new Docker image.
1. Deploy SFMon to your ECS cluster using Terraform.

Ensure that you create a backend.tf file in sre/deployments/terraform/sfmon-service/ecs-service with the appropriate state file configuration.

```
# Set environment variable for Docker image tag
TF_VAR_docker_image_tag="$CI_COMMIT_SHORT_SHA"

cd sre/deployments/terraform/sfmon-service/ecs-service
terraform init -input=false
terraform apply -input=false -auto-approve
```

## Grafana

The `configs/grafana` directory contains a sample Grafana dashboard JSON file. You can import this into your Grafana instance and customize it based on your orgs and alerting needs.

## Notes

- Each Salesforce org to be monitored must have a corresponding SFDX AUTH URL.
    - These are generated via `sf org display --target-org {alias} --verbose`
- ‚ö†Ô∏è **Security Note:** Do not commit or expose your SFDX AUTH URLs in source control. Use environment variables or secrets management solutions (e.g., AWS Secrets Manager) to handle credentials securely.
- This setup assumes Prometheus is scraping from the same ECS cluster where SFMon is deployed.

## Alternatives

While this repo focuses on deploying SFMon to AWS ECS, the core functionality is portable.

The heart of this repository is:

- The Python monitoring scripts (`sre/deployments/scripts/sfmon-service`)
- The Dockerfile (`sre/deployments/docker/sfmon-service`)

These can be reused in other environments:

- **Google Kubernetes Engine (GKE)**: Add a `PodMonitor` or use scraping annotations.
- **Google Cloud Run / VMs**: Run a Prometheus agent or OpenTelemetry Collector to scrape the container‚Äôs metrics endpoint and forward data to Google Cloud Monitoring or another backend.

